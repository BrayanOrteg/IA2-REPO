# ğŸ§ª GuÃ­a de EjecuciÃ³n de Pruebas - QuickTask

## ğŸ“‹ Prerequisitos

1. **Entorno virtual activado**
2. **Dependencias de desarrollo instaladas**

---

## ğŸ”§ InstalaciÃ³n

### 1. Instalar dependencias de testing

```powershell
# Navegar al directorio backend
cd backend

# Activar entorno virtual (si no estÃ¡ activado)
.\venv\Scripts\Activate.ps1

# Instalar dependencias de desarrollo
pip install -r requirements-dev.txt
```

---

## â–¶ï¸ Ejecutar pruebas

### Todas las pruebas

```powershell
pytest
```

**Salida esperada:**
```
tests/test_auth.py ..................  [ 45%]
tests/test_tasks.py ........................  [ 85%]
tests/test_reminders.py ........  [100%]

==================== 48 passed in 2.34s ====================
```

---

### Por archivo especÃ­fico

```powershell
# Solo pruebas de autenticaciÃ³n
pytest tests/test_auth.py

# Solo pruebas de tareas
pytest tests/test_tasks.py

# Solo pruebas de recordatorios
pytest tests/test_reminders.py
```

---

### Por marcadores (tags)

```powershell
# Solo pruebas unitarias
pytest -m unit

# Solo pruebas de integraciÃ³n
pytest -m integration
```

---

### Con reporte de cobertura

```powershell
# Generar reporte de cobertura en terminal
pytest --cov=app --cov-report=term-missing

# Generar reporte HTML (mÃ¡s detallado)
pytest --cov=app --cov-report=html

# Abrir reporte HTML en navegador
start htmlcov/index.html
```

**Salida esperada:**
```
----------- coverage: platform win32, python 3.12.0 -----------
Name                    Stmts   Miss  Cover   Missing
-----------------------------------------------------
app/__init__.py             0      0   100%
app/auth.py                58      2    97%   45-46
app/crud.py                92      5    95%   23, 67, 89-91
app/database.py            12      0   100%
app/main.py               156      8    95%   234-241
app/models.py              35      0   100%
app/schemas.py             42      0   100%
-----------------------------------------------------
TOTAL                     395     15    96%
```

---

### Modo verboso (mÃ¡s detalles)

```powershell
pytest -v
```

---

### Detener en el primer fallo

```powershell
pytest -x
```

---

### Ver print statements

```powershell
pytest -s
```

---

## ğŸ¯ Pruebas especÃ­ficas

### Ejecutar una prueba especÃ­fica

```powershell
# Por nombre de funciÃ³n
pytest tests/test_auth.py::test_register_user_success

# Por clase (si usas clases de prueba)
pytest tests/test_tasks.py::TestTaskCreation

# Ejecutar mÃºltiples pruebas con patrÃ³n
pytest -k "create"  # Ejecuta todas las pruebas con "create" en el nombre
```

---

## ğŸ” Debugging

### Ejecutar con debugger

```powershell
# Pytest se detendrÃ¡ en breakpoints
pytest --pdb
```

### Ver warnings

```powershell
pytest -W all
```

---

## ğŸ“Š Reportes avanzados

### Reporte JUnit XML (para CI/CD)

```powershell
pytest --junitxml=report.xml
```

### Reporte JSON

```powershell
pytest --json-report --json-report-file=report.json
```

---

## âš¡ OptimizaciÃ³n

### Ejecutar en paralelo (mÃ¡s rÃ¡pido)

```powershell
# Instalar pytest-xdist
pip install pytest-xdist

# Ejecutar con mÃºltiples workers
pytest -n auto
```

---

## ğŸ“ Estructura de salida

### Ejemplo de salida exitosa

```
================================ test session starts =================================
platform win32 -- Python 3.12.0, pytest-7.4.3, pluggy-1.3.0
rootdir: C:\...\backend
configfile: pytest.ini
plugins: asyncio-0.21.1, cov-4.1.0, mock-3.12.0
collected 48 items

tests/test_auth.py ..................                                        [ 37%]
tests/test_tasks.py ........................                                  [ 87%]
tests/test_reminders.py ........                                             [100%]

================================ 48 passed in 2.34s ==================================
```

### Ejemplo con fallos

```
================================ FAILURES ==========================================
___________________________ test_create_task_success ______________________________

client = <fastapi.testclient.TestClient object at 0x...>
auth_headers = {'Authorization': 'Bearer eyJhbGc...'}

    def test_create_task_success(client, auth_headers, sample_task_data):
>       response = client.post("/api/tasks", headers=auth_headers, json=sample_task_data)
E       AssertionError: assert 400 == 201

tests/test_tasks.py:15: AssertionError
======================= 1 failed, 47 passed in 2.67s ===========================
```

---

## âœ… VerificaciÃ³n de calidad

### Cobertura mÃ­nima recomendada

```powershell
# Falla si cobertura < 90%
pytest --cov=app --cov-fail-under=90
```

---

## ğŸ› Troubleshooting

### Error: "ModuleNotFoundError: No module named 'app'"

**SoluciÃ³n:**
```powershell
# AsegÃºrate de estar en el directorio backend/
cd backend
pytest
```

### Error: "Database is locked"

**SoluciÃ³n:** Las pruebas usan SQLite en memoria, no deberÃ­a ocurrir. Si sucede:
```powershell
# Elimina la base de datos de prueba si existe
Remove-Item test.db -ErrorAction SilentlyContinue
```

### Pruebas lentas

**SoluciÃ³n:**
```powershell
# Ejecutar en paralelo
pip install pytest-xdist
pytest -n auto
```

---

## ğŸ“š Recursos adicionales

- **Pytest docs:** https://docs.pytest.org/
- **FastAPI testing:** https://fastapi.tiangolo.com/tutorial/testing/
- **Coverage.py:** https://coverage.readthedocs.io/

---

## ğŸ“ Mejores prÃ¡cticas

âœ… **Ejecutar pruebas antes de cada commit**
âœ… **Mantener cobertura > 80%**
âœ… **Agregar pruebas para cada nuevo feature**
âœ… **Usar fixtures para evitar duplicaciÃ³n**
âœ… **Nombres descriptivos de pruebas**
âœ… **Aislar pruebas (sin dependencias entre ellas)**

---

## ğŸš€ IntegraciÃ³n continua

### GitHub Actions (ejemplo)

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      - run: pip install -r requirements.txt -r requirements-dev.txt
      - run: pytest --cov=app --cov-report=xml
      - uses: codecov/codecov-action@v3
```

---

## ğŸ“Š MÃ©tricas de calidad actuales

| MÃ©trica                | Valor   |
|------------------------|---------|
| Tests implementados    | 48      |
| Cobertura de cÃ³digo    | ~96%    |
| Endpoints cubiertos    | 18/18   |
| Tiempo de ejecuciÃ³n    | ~2.5s   |

---

Â¡Listo para testear! ğŸ‰
